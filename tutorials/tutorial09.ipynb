{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE9010: Introduction to Data Science\n",
    "## Semester 2 2017/18\n",
    "## Xavier Bresson\n",
    "<hr>\n",
    "\n",
    "## Tutorial 5: Supervised classification - improving capacity learning\n",
    "## Objectives\n",
    "### $\\bullet$ Code linear and higher-order logistic regression models\n",
    "### $\\bullet$ Explore results\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# math library\n",
    "import numpy as np\n",
    "\n",
    "# visualization library\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 3d visualization\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# computational time\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load dataset #1\n",
    "<hr>\n",
    "The data features for each data $i$ are $x_i=(x_{i(1)},x_{i(2)})$. <br>\n",
    "The data label/target, $y_i$, indicates two classes with value 0 or 1.\n",
    "\n",
    "Plot the data points.<br>\n",
    "Hint: You may use matplotlib function `scatter(x,y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data with numpy\n",
    "data = np.loadtxt('data/two_circles.txt', delimiter=',')\n",
    "\n",
    "# number of training data\n",
    "n = data.shape[0] \n",
    "print('Number of training data=',n)\n",
    "\n",
    "# print\n",
    "print(data[:10,:])\n",
    "print(data.shape)\n",
    "print(data.dtype)\n",
    "\n",
    "# plot\n",
    "x1 = data[:,0] # feature 1\n",
    "x2 = data[:,1] # feature 2\n",
    "idx_class0 = (data[:,2]==0) # index of class0\n",
    "idx_class1 = (data[:,2]==1) # index of class1\n",
    "\n",
    "plt.figure(1,figsize=(6,6))\n",
    "plt.#YOUR CODE HERE\n",
    "plt.#YOUR CODE HERE\n",
    "plt.title('Training data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linear logistic regression/classification task.\n",
    "<hr>\n",
    "\n",
    "The logistic regression/classification predictive function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_w(x) &= \\sigma(X w)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the case of **linear** prediction, we have:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "X = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "1 & x_{1(1)} & x_{1(2)} \\\\ \n",
    "1 & x_{2(1)} & x_{2(2)} \\\\ \n",
    "\\vdots\\\\\n",
    "1 & x_{n(1)} & x_{n(2)} \n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "w = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\Rightarrow \n",
    "\\quad\n",
    "p_w(x) = \\sigma(X w)  =\n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "\\sigma(w_0 + w_1 x_{1(1)} + w_2 x_{1(2)}) \\\\ \n",
    "\\sigma(w_0 + w_1 x_{2(1)} + w_2 x_{2(2)}) \\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma(w_0 + w_1 x_{n(1)} + w_2 x_{n(2)})\n",
    "\\end{array} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Implement the linear logistic regression function with gradient descent or scikit-learn. Visualize the boundary decision.<br>\n",
    "\n",
    "Check your code correctness: The loss value should be around 0.693. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute values p(x) for multiple data points x\n",
    "x1_min, x1_max = X[:,1].min(), X[:,1].max() # min and max of grade 1\n",
    "x2_min, x2_max = X[:,2].min(), X[:,2].max() # min and max of grade 2\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max)) # create meshgrid\n",
    "X2 = np.ones([np.prod(xx1.shape),3]) \n",
    "X2[:,1] = xx1.reshape(-1)\n",
    "X2[:,2] = xx2.reshape(-1)\n",
    "p = f_pred(X2,w)\n",
    "p = p.reshape(xx1.shape)\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(4,figsize=(6,6))\n",
    "plt.scatter(x1[idx_class0], x2[idx_class0], s=60, c='r', marker='+', label='Class0') \n",
    "plt.scatter(x1[idx_class1], x2[idx_class1], s=30, c='b', marker='o', label='Class1')\n",
    "plt.contour(xx1, xx2, p, [0.5], linewidths=2, colors='k') \n",
    "plt.legend()\n",
    "plt.title('Decision boundary (linear)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Quadratic logistic regression/classification task.\n",
    "<hr>\n",
    "\n",
    "The logistic regression/classification predictive function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_w(x) &= \\sigma(X w)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the case of **quadratic** prediction, we have:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "X = \n",
    "\\left[ \n",
    "\\begin{array}{cccccc}\n",
    "1 & x_{1(1)} & x_{1(2)} & x_{1(1)}^2 & x_{1(2)}^2 & x_{1(1)}x_{1(2)} \\\\ \n",
    "1 & x_{2(1)} & x_{2(2)} & x_{2(1)}^2 & x_{2(2)}^2 & x_{2(1)}x_{2(2)}\\\\ \n",
    "\\vdots\\\\\n",
    "1 & x_{n(1)} & x_{n(2)} & x_{n(1)}^2 & x_{n(2)}^2 & x_{n(1)}x_{n(2)}\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "w = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2\\\\ \n",
    "w_3\\\\ \n",
    "w_4\\\\ \n",
    "w_5\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "Implement the quadratic logistic regression function with gradient descent or scikit-learn. Visualize the boundary decision.<br>\n",
    "\n",
    "Check your code correctness: The loss value should be around 0.011. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load dataset #2\n",
    "<hr>\n",
    "The data features for each data $i$ are $x_i=(x_{i(1)},x_{i(2)})$. <br>\n",
    "The data label/target, $y_i$, indicates two classes with value 0 or 1.\n",
    "\n",
    "Plot the data points.<br>\n",
    "Hint: You may use matplotlib function `scatter(x,y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data with numpy\n",
    "data = np.loadtxt('data/two_moons.txt', delimiter=',')\n",
    "\n",
    "# number of training data\n",
    "n = data.shape[0] \n",
    "print('Number of training data=',n)\n",
    "\n",
    "# print\n",
    "print(data[:10,:])\n",
    "print(data.shape)\n",
    "print(data.dtype)\n",
    "\n",
    "# plot\n",
    "x1 = data[:,0] # feature 1\n",
    "x2 = data[:,1] # feature 2\n",
    "idx_class0 = (data[:,2]==0) # index of class0\n",
    "idx_class1 = (data[:,2]==1) # index of class1\n",
    "\n",
    "plt.figure(1,figsize=(6,6))\n",
    "plt.#YOUR CODE HERE\n",
    "plt.#YOUR CODE HERE\n",
    "plt.title('Training data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Linear logistic regression/classification task.\n",
    "<hr>\n",
    "\n",
    "\n",
    "Implement the linear logistic regression function with gradient descent or scikit-learn. Visualize the boundary decision.<br>\n",
    "\n",
    "Check your code correctness: The loss value should be around 0.255. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Quadratic logistic regression/classification task.\n",
    "<hr>\n",
    "\n",
    "\n",
    "Implement the quadratic logistic regression function with gradient descent or scikit-learn. Visualize the boundary decision.<br>\n",
    "\n",
    "Check your code correctness: The loss value should be around 0.255. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Cubic logistic regression/classification task.\n",
    "<hr>\n",
    "\n",
    "The logistic regression/classification predictive function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_w(x) &= \\sigma(X w)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the case of **cubic** prediction, we have:\n",
    "\n",
    "<br>\n",
    "$$\n",
    "X = \n",
    "\\left[ \n",
    "\\begin{array}{cccccccc}\n",
    "1 & x_{1(1)} & x_{1(2)} & x_{1(1)}^2 & x_{1(2)}^2 & x_{1(1)}x_{1(2)} & x_{1(2)}^3 & x_{1(2)}^3 & x_{1(1)}^2x_{1(2)} & x_{1(1)}x_{1(2)}^2 \\\\ \n",
    "1 & x_{2(1)} & x_{2(2)} & x_{2(1)}^2 & x_{2(2)}^2 & x_{2(1)}x_{2(2)} & x_{2(2)}^3 & x_{2(2)}^3 & x_{2(1)}^2x_{2(2)} & x_{2(1)}x_{2(2)}^2\\\\ \n",
    "\\vdots\\\\\n",
    "1 & x_{n(1)} & x_{n(2)} & x_{n(1)}^2 & x_{n(2)}^2 & x_{n(1)}x_{n(2)} & x_{n(2)}^3 & x_{n(2)}^3 & x_{n(1)}^2x_{n(2)} & x_{n(1)}x_{n(2)}^2\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "\\textrm{ and }\n",
    "\\quad\n",
    "w = \n",
    "\\left[ \n",
    "\\begin{array}{cccc}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2\\\\ \n",
    "w_3\\\\ \n",
    "w_4\\\\ \n",
    "w_5\\\\ \n",
    "w_6\\\\ \n",
    "w_7\\\\ \n",
    "w_8\\\\ \n",
    "w_9\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "Implement the cubic logistic regression function with gradient descent or scikit-learn. Visualize the boundary decision.<br>\n",
    "\n",
    "Check your code correctness: The loss value should be around 0.043. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
